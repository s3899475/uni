Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2023-03-06T11:28:58+11:00

====== Week 2 - estimating the running time ======
Contents:
[*] [[#Part 1: Overview|Part 1: Overview]]
[*] [[#Part 2: Fundamentals|Part 2: Fundamentals]]
[*] [[#Part 3: Asymptotic Complexity|Part 3: Asymptotic Complexity]]
[*] [[#Part 4: Complexity Analysis for Non-Recursive Algorithms|Part 4: Complexity Analysis for Non-Recursive Algorithms]]
[*] [[#Summation Rules|Summation Rules]]

===== Part 1: Overview =====
Obvious solution to compare algorithms - run algorithm on a certain data set and record the time it takes

=== Problems: ===
- different sized datasets can have a big difference
- also countless different datasets (real world data will probably not be random as well)
- many algorithms out there, would need to test each one

**Solution: Big O & friends (estimation)** - can talk about speed of algorithms without implementing them - can therefore compare them

=== 2 types of analysis: ===
non-recursive - this week
recursive - next week

It is vital to analyse the resource usage of an algorithm, well before it is implemented and deployed - week 11

===== Part 2: Fundamentals =====
**Estimation of the running time of an algorithm**
An Algorithm consists of some operations executed a number of times
Estimate can be obtained through:
* Determining operations
* How long to execute each operation
* Number of times they are executed

^^^ called **basic operations**, time is based on **input size**

=== **basic operations:** ===
* Operations that contribute the most to the total running time
	* e.g. compare, add, multiply, divide, assignment
* Typically most frequently executed, although dependent on the time of each operation

=== Input size ===
* characteristic of the problem 'size of the problem'
	* e.g. searching through an array of size n, the input size is 'n'
* need to estimate the running time in terms of the problem - relative terms
* state running time (number of times the basic operation is executed) in terms of the input size

=== e.g. pseudo-code: ===
''// INPUT a, n''
''// OUTPUT: s = a''^{''n''}
''for i = 1 to n do''
	''s = s * a''
''end for''
''return s''

n and a are the input, n is the input size because it is the number of iterations it runs

==== estimation of running time ====
running time is approximately equal to time to execute a basic operation × number of basic operations
{{./equation.png?type=equation}}
* t(n) is the running time
* n is the input size
* C_{op} is the execution time for a basic operation
* C(n) is the number of times the basic operation is executed

=== e.g. calculation of running time for pseudo-code ===
multiplication is the only basic operation
we don't care about non-basic operations
C(n) = n in for loop

==== complexity cases ====
worst case - maximum running time
best case - minimum running time
average case, **average over all possible inputs**

=== e.g. pseudo-code 2 ===
'''
//INPUT: An array of length n and search key K
// OUTPUT: The index of the first element of A which
// matches K or n (length of A) otherwise
set i = 0
while i < n and A[i] ≠ K do
	set i = i + i
end while
return i
'''
* basic operation: comparison, addition, assignment (any of these are okay)
* input size: n
* best case: C_{b} = 1
* worst case: C_{w} = n or n + 1 (constant added doesn't matter)
* average case - average across all inputs, typically not straightforward
	* need to know how often we search for items in the array and also how often we search for items not in the array!

====== Need to watch derivation on canvas ======
average case analysis for pseudocode:
{{./equation001.png?type=equation}}
where p(n+1) is the probability of a successful search


===== Part 3: Asymptotic Complexity =====
Problem: how to compare running times in a meaningful way?
Solution:
* Group algorithms into **equivalence classes**, with respect to input size (n)

Asymptotic complexity:
* When n becomes large, what is the dominant term contributing to the running time?
=== = ===
given function t(n) (running time of an algorithm)
* let c × g(n) be a function that is an upper bound on t(n) for some c > 0 and "large" n
	* n_{0} = large n (lowest n that we care about, can be anything. e.g. 100, 1000, etc.)

==== Big-O notation (upper bound) ====
{{./pasted_image003.png}}
* O(n): given a function t(n)
	* Informal: t(n) ∈ O(g(n)) means g(n) is a function that, as n increases, provides an upper bound for t(n)
	* Formal: t(n) ∈ O(g(n)), if g(n) is a function and c × g(n) is an upper bound on t(n) for some c > 0 and for "large" n

=== e.g. ===
if t(n) = 5.1n
* g(n) = n
	* c could be 6 in this instance, because 6n is always ≥ 5.1n when n > 0
	* therefore, t(n) ∈ O(n)
* g(n) = 0.001n - 6
	* c could be 6000, because 6n - 36,000 is always ≥ 5.1n when n is big enough
	* therefore, t(n) ∈ O(0.001n - 6)
* g(n) = n^{2}
	* c could be 1, because n^{2} is always ≥ n when n is anything
	* therefore, t(n) ∈ O(n^{2})
these functions are all true if t(n) = 5.2n, 5.3n, etc.

==== Equivalence classes ====
| Time                       | Big-O       | Example                     |
|:---------------------------|:------------|:----------------------------|
| Constant                   | O(1)        | Access array element        |
| Logarithmic                | O(log(n))   | Binary Search               |
| Linear                     | O(n)        | Linked List Search          |
| Linearithmic (Supralinear) | O(n log(n)) | Merge Sorting               |
| Quadratic                  | O(n ^ 2)    | Selection Sorting           |
| Exponential                | O(2 ^ n)    | Generating all subsets      |
| Factorial                  | O(n!)       | Generating all permutations |

To find the **equivalence function class** that upper bounds different t(n):
* Do not include constants (n - 5 → n)
* Omit all the lower ordered terms (n^{2} + n → n^{2})
* Omit the coefficient of the highest order term (6n → n)


==== Lower bounds (Ω(n)) ====
{{./pasted_image002.png}}
* Informal: t(n) ∈ Ω(g(n)) means g(n) is a function that, as n increases, is a lower bound of t(n)
* Formal: t(n) ∈ Ω(g(n)), is g(n) is a function and c × g(n) is a lower bound on t(n) for some c > 0 and for "large" n

==== Exact bounds (Θ(n)) ====
{{./pasted_image001.png}}
* Informal: t(n) ∈ Θ(g(n)) means g(n) is a function that, as n increases, is both an upper and lower bound of t(n)
* Formal: t(n) ∈ Θ(g(n)), is g(n) is a function and c_{1} × g(n) is an upper bound on t(n) and c_{2} × g(n) is a lower bound on t(n), for some c_{1} > 0 and c_{2} > 0 and for "large" n.

==== O(n), Θ(n) and Ω(n) are not the same as worst, best and average case efficiency, respecively ====

===== Part 4: Complexity Analysis for Non-Recursive Algorithms =====
{{./equation002.png?type=equation}}
can usually only care about C(n), as c_{op} is a constant
=== Determine bounds on the order of growth of an algorithm ===
* Determine what is the input size (n)
* Identify the basic operation
* Determine the number of basic executions in terms of n (i.e. C(n))
	* Setup as summation for C(n), reflecting the algorithm's loop structure
	* Simplify the summation using standard formulas in Appendix A of textbook

=== e.g. pseudocode ===
''// INPUT: a, n''
''// OUTPUT: s = a''^{''n''}
''set s = 1''
''for i = 1 to n do''
	''s = s * a''
''end for''
''return''
* input size: n
* basic operation: multiplication
* C(n): 1 + 1 + 1 + ... + 1 =
{{./equation003.png?type=equation}}
simplify (using below summation rules)
{{./equation007.png?type=equation}}
{{./equation008.png?type=equation}}

=== e.g. code 2 ===
Add two square matrices, of dimensions n × n
''for (int i=0; i <= n-1; i++) {''
	''for (int j=0; j<= n-1; j++) {''
		''C[i, j] = A[i, j] + B[i, j];''
	''}''
''}''
* input size: n
* Basic operation: addition
* C(n):
{{./equation004.png?type=equation}}
simplify inner sum:
{{./equation005.png?type=equation}}
simplify:
{{./equation009.png?type=equation}}
{{./equation006.png?type=equation}}

===== Summation rules =====
{{./pasted_image004.png}}


